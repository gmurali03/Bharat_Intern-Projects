# -*- coding: utf-8 -*-
"""House Price Pridiction Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RMpQtRooDbal6t9XRXA7mP5ROybKMAQI
"""

import pandas as pd #importing pandas library

Data = pd.read_excel('/content/DS - Assignment Part 1 data set.xlsx') #importing Dataset in xlsx format
Data.head()

Data.drop(["Transaction date"],axis = 1,inplace = True)
Data

Data.columns

Data.isna().sum() #checking for none values

import seaborn as sns
sns.boxplot(Data['House Age'])

Data.plot(kind = 'box',rot=45,figsize=(10, 8)) #checking outlier through box plot

sns.boxplot(Data['Distance from nearest Metro station (km)'])

"""Using Inter quantile range formula for removing outlier"""

Q1 = Data['Distance from nearest Metro station (km)'].quantile(0.25)

Q3 = Data['Distance from nearest Metro station (km)'].quantile(0.75)

IQR = Q3 - Q1
IQR

outliers1 = Data[(Data['Distance from nearest Metro station (km)'] <(Q1 - 1.5*IQR)) | (Data['Distance from nearest Metro station (km)'] > (Q3
+ 1.5*IQR))]

outliers1.index

Data.drop(outliers1.index, inplace = True, axis = 0)

Data['Distance from nearest Metro station (km)'].plot(kind = 'box')

Data['latitude'].plot(kind = 'box')

Q11 = Data['latitude'].quantile(0.25)
Q13 = Data['latitude'].quantile(0.75)
IQR2 = Q13 - Q11
IQR2
outliers2 = Data[(Data['latitude'] <(Q11 - 1.5*IQR2)) | (Data['latitude'] > (Q13
+ 1.5*IQR2))]
outliers2

outliers2.index
Data.drop(outliers2.index, inplace = True, axis = 0)
Data['latitude'].plot(kind = 'box')

Data['longitude'].plot(kind = 'box')

Q21 = Data['longitude'].quantile(0.25)
Q23 = Data['longitude'].quantile(0.75)
IQR3 = Q23 - Q21
outliers3 = Data[(Data['longitude'] <(Q21 - 1.5*IQR3)) | (Data['longitude'] > (Q23
+ 1.5*IQR3))]
outliers3.index
Data.drop(outliers3.index, inplace = True, axis = 0)
Data['longitude'].plot(kind = 'box')

Data['House price of unit area'].plot(kind = 'box')

Q31 = Data['House price of unit area'].quantile(0.25)
Q33 = Data['House price of unit area'].quantile(0.75)
IQR4 = Q33 - Q31
outliers4 = Data[(Data['House price of unit area'] <(Q31 - 1.5*IQR4)) | (Data['House price of unit area'] > (Q33
+ 1.5*IQR4))]
outliers4.index
Data.drop(outliers4.index, inplace = True, axis = 0)
Data['House price of unit area'].plot(kind = 'box')

#Here we are using corelation plot for checking the relation between the feature

import matplotlib.pyplot as plt

corr=Data.corr()
top_features=corr.index
plt.figure(figsize=(16,8))
sns.heatmap(Data[top_features].corr(),annot=True)

# Number of bedrooms and House size has highest corelation so i drop one of them

Data.drop(['Number of bedrooms'],axis =1,inplace = True)

corr=Data.corr()
top_features=corr.index
plt.figure(figsize=(16,8))
sns.heatmap(Data[top_features].corr(),annot=True)

import plotly.express as px

Data.columns

fig = px.bar(Data, x='House Age', y='House price of unit area')
fig.show()

fig = px.bar(Data, x='Number of convenience stores', y='House price of unit area')
fig.show()

# Number of convenience store has highest house price per unit area

sns.lineplot(data=Data, x="latitude", y="House price of unit area")

sns.lineplot(data=Data, x="longitude", y="House price of unit area")

Data.columns

# spliting the data into X and y. X is independent variable and y id dependent variable.
# By using sklearn library split the data into test and train.

X = Data[['House Age', 'Distance from nearest Metro station (km)',
       'Number of convenience stores', 'latitude', 'longitude',
       'House size (sqft)']]
y = Data['House price of unit area']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)
regressor.fit(X_train, y_train)
from sklearn.metrics import mean_squared_error as MSE
import numpy as np
pred1=regressor.predict(X_test)
print('RMSE values :',np.sqrt(MSE(y_test,pred1)))
import sklearn.metrics as metrics
print('MAE:', metrics.mean_absolute_error(y_test, pred1))
print('MSE',metrics.mean_squared_error(y_test,pred1))

from sklearn.ensemble import AdaBoostRegressor
abr = AdaBoostRegressor(random_state=0, n_estimators=100)
abr.fit(X_train,y_train)
pred2=abr.predict(X_test)
print('RMSE values :',np.sqrt(MSE(y_test,pred2)))
print('MAE:', metrics.mean_absolute_error(y_test, pred2))
print('MSE',metrics.mean_squared_error(y_test,pred2))

from sklearn.svm import SVR
svr = SVR()
svr.fit(X_train, y_train)
pred3=svr.predict(X_test)
print('RMSE values :',np.sqrt(MSE(y_test,pred3)))
print('MAE:', metrics.mean_absolute_error(y_test, pred3))
print('MSE',metrics.mean_squared_error(y_test,pred3))

from sklearn.neighbors import KNeighborsRegressor
knr = KNeighborsRegressor()
knr.fit(X_train,y_train)
pred4=knr.predict(X_test)
print(pred4)
print('RMSE values :',np.sqrt(MSE(y_test,pred4)))
print('MAE:', metrics.mean_absolute_error(y_test, pred4))
print('MSE',metrics.mean_squared_error(y_test,pred4))

DF = {'Algorithm':['R-Forest', 'AdaBst', 'SVR','KNR'],
        'MSE':[56.27,59.67,85.27,86.50]}

DF1 = pd.DataFrame(DF)
DF1

sns.factorplot(x ='Algorithm', y ='MSE', data = DF1)

# Random forest has minimum error so Random forest is Best Suitable Algorithm